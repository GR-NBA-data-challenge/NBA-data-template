{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libsimulation\n",
    "from src import main\n",
    "\n",
    "import os, datetime, argparse, requests, urllib.parse, sys, re, traceback, json\n",
    "import math, numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA data jupyter notebook\n",
    "\n",
    "__Important__\n",
    "This notebook is here for you to quickly test with the data.\n",
    "It is __not__ the final submission, as we will only run your code provided in `src/main.py` and any other files referenced from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some environmental settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some settings\n",
    "settings = libsimulation.SimulationSettings()\n",
    "settings.env = 'prod'\n",
    "## Ensure we don't use data beyond this cutoff\n",
    "settings.cutoff = '2019-01-01'\n",
    "settings.resultpath = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a basic predict method that generates silly, cosntant, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code\n",
    "def predict(required_predictions, data_loader):\n",
    "    # Load games data for the 2011 season.\n",
    "    # Seasons from 2009 onwards are available, including POST seasons, such as 2011POST\n",
    "    games2011 = data_loader.getSeason('2011')\n",
    "    print(f'Loaded {len(games2011)} 2011 games')\n",
    "    print(f'First entry in 2011 season is\\n{games2011.iloc[[0]]}')\n",
    "\n",
    "    # Loading a season that is ahead of the cutoff training time returns no results.\n",
    "    # In this case, the default cutoff time is in 2019, so loading 2020 data returns no results.\n",
    "    # You can change the cutoff time by passing to simulate.py\n",
    "    #     --cutoff YYYY-MM-DD\n",
    "    games2020 = data_loader.getSeason('2020')\n",
    "    print(f'Loaded {len(games2020)} 2020 games')\n",
    "\n",
    "    # You can load an individual match's data\n",
    "    aGame = data_loader.getGame(games2011.loc[100, 'gameId'])\n",
    "    print(f'Game 100 in the 2011 database is\\n{aGame}')\n",
    "\n",
    "    # You can also load the full players data for an entire season\n",
    "    full2011seasonPlayers = data_loader.getPlayers('2011')\n",
    "    print(f'Loaded {len(full2011seasonPlayers)} 2011 season player rows')\n",
    "\n",
    "    print(f'Required predictions are\\n{required_predictions}')\n",
    "\n",
    "    # You should fill in the sum and diff fields of the required predictions\n",
    "    for index, match in required_predictions.iterrows():\n",
    "        required_predictions.at[index, 'sum'] = 999\n",
    "        required_predictions.at[index, 'diff'] = 1\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run a simulation\n",
    "settings.predict = predict\n",
    "libsimulation.runSimulation(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You can also run the code present in src/main.py\n",
    "settings.predict = main.predict\n",
    "libsimulation.runSimulation(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = libsimulation.NbaDataLoader(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.getSeason('2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.getSeason('2018POST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.getGame(5210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.getPlayers('2011')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some data into train and validaiton sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = libsimulation.NbaDataLoader(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_season_game_data(data_loader, first_year, last_year):\n",
    "    data = [pd.DataFrame(data_loader.getSeason(str(season))) for season in range(first_year, last_year + 1)]\n",
    "    data = pd.concat(data, axis=0)\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    data.dateTime=pd.to_datetime(data.dateTime)\n",
    "    data.sort_values('dateTime', inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_multi_season_game_data(data_loader, 2009, 2016)\n",
    "test_data = get_multi_season_game_data(data_loader, 2017, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions to calcuate a Elo ratings over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elo model's probability of home team winning\n",
    "def home_win_probability(home_elo, away_elo):\n",
    "    return 1 / (1 + math.pow(10, -(home_elo - away_elo) / 400)) \n",
    "\n",
    "## Get new Elo ratings home and away teams after a game\n",
    "def get_updated_elo(\n",
    "    home_elo, away_elo, \n",
    "    home_victory, ## 1 if home team won, 0 if away team won\n",
    "    K,  ## model hyperparameter \n",
    "): \n",
    "    if home_victory not in [0, 1, False, True]:\n",
    "        raise ValueError(f\"home_victory should be 1 if home team won, 0 if away team won. Got {home_victory}\")\n",
    "\n",
    "    P_home_win = home_win_probability(home_elo, away_elo) \n",
    "    P_away_win = 1 - P_home_win\n",
    "  \n",
    "    # When home team wins \n",
    "    if home_victory : \n",
    "        home_elo += K * P_away_win\n",
    "        away_elo -= K * P_home_win\n",
    "      \n",
    "    # When away team wins \n",
    "    else : \n",
    "        home_elo -= K * P_away_win\n",
    "        away_elo += K * P_home_win\n",
    "        \n",
    "    return home_elo, away_elo\n",
    "\n",
    "## Iterate through games updating each teams Elo rating\n",
    "def get_elos_over_time(data, ## dataframe of games, must be in order of occurence\n",
    "                      starting_elo_dict={},  ## dictionary of elo scores by team at the beginning of the data period\n",
    "                      default_elo=0,  ## elo initally given to a team not in starting_elo_dict\n",
    "                      K=10,  ## model hyperparameter; higher number means individuals game affects Elo more\n",
    "                     ):\n",
    "    \n",
    "    elo_dict = starting_elo_dict.copy()\n",
    "    data['homeElo'] = np.nan\n",
    "    data['awayElo'] = np.nan\n",
    "\n",
    "    ## Iterate over rows of the dataframe (i.e. over games)\n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        home_team = row['homeTeam']\n",
    "        away_team = row['awayTeam']\n",
    "        home_elo = elo_dict.get(home_team, default_elo)\n",
    "        away_elo = elo_dict.get(away_team, default_elo)\n",
    "        \n",
    "        ## Put the team's current ELO in the dataframe (this is the teams ELO *before* the match)\n",
    "        data.loc[i,'homeElo'] = home_elo\n",
    "        data.loc[i,'awayElo'] = away_elo\n",
    "        \n",
    "        ## Calculate the new elo scores and update elo_dict with them\n",
    "        home_victory = row['pointsDiff'] > 0\n",
    "        home_elo, away_elo = get_updated_elo(home_elo, away_elo, home_victory, K)\n",
    "        elo_dict[home_team] = home_elo\n",
    "        elo_dict[away_team] = away_elo\n",
    "    \n",
    "    return elo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "pre_test_elo_dict = get_elos_over_time(train_data, starting_elo_dict={}, K=K)\n",
    "post_test_elo_dict = get_elos_over_time(test_data, starting_elo_dict=pre_test_elo_dict, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Elo ratings over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_team_elo_over_time(data, team):\n",
    "    team_data = data.query(f'homeTeam == \"{team}\" | awayTeam == \"{team}\"').copy()\n",
    "    team_data['Elo'] = team_data.eval(f'(homeTeam == \"{team}\") * homeElo + (awayTeam == \"{team}\") * awayElo')\n",
    "    team_data = team_data[['dateTime', 'gameId', 'Elo']]\n",
    "    plt.plot(team_data['dateTime'], team_data['Elo'], label=team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([train_data, test_data])\n",
    "for team in ['GS', 'MIA', 'NY', 'SA']:\n",
    "    plot_team_elo_over_time(combined_data, team)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a linear model on our train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['EloDifference'] = train_data['homeElo'] - train_data['awayElo']\n",
    "test_data['EloDifference'] = test_data['homeElo'] - test_data['awayElo']\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X=train_data[['EloDifference']], y=train_data['pointsDiff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the statsmodels library to fit a linear model of Elo difference to points difference\n",
    "X = train_data['EloDifference']\n",
    "y = train_data['pointsDiff']\n",
    "model = sm.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X=train_data[['EloDifference']], y=train_data['pointsSum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the statsmodels library to fit a linear model of Elo difference to points difference\n",
    "train_data['EloSum'] = train_data['homeElo'] + train_data['awayElo']\n",
    "test_data['EloSum'] = test_data['homeElo'] + test_data['awayElo']\n",
    "X = train_data[['EloDifference', 'EloSum']]\n",
    "X = sm.add_constant(X)\n",
    "y = train_data['pointsDiff']\n",
    "model = sm.OLS(y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predicitons on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[['EloDifference', 'EloSum']]\n",
    "X_test = sm.add_constant(X_test)\n",
    "test_data['predictedDiff'] = model.predict(X_test)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check how good our predictions are\n",
    "sns.lmplot('predictedDiff', 'pointsDiff', test_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remodel to see statistics on test data\n",
    "X = test_data['predictedDiff']\n",
    "y = test_data['pointsDiff']\n",
    "test_model = sm.OLS(y, X).fit()\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a prediction method to incorporate the Elo model and return valid predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code\n",
    "def predict(required_predictions, data_loader, log=lambda x: print(x)):\n",
    "    first_year = 2016\n",
    "    \n",
    "    log('Loading training data')\n",
    "    train_data = get_multi_season_game_data(data_loader, first_year=first_year, last_year=2020)\n",
    "    \n",
    "    log('Getting Elo ratings over time on train data')\n",
    "    elo_dict = get_elos_over_time(train_data, starting_elo_dict={}, K=10)\n",
    "    train_data['EloDifference'] = train_data['homeElo'] - train_data['awayElo']\n",
    "    train_data['EloSum'] = train_data['homeElo'] + train_data['awayElo']\n",
    "    \n",
    "    log('Fitting linear model from Elo difference and sum to points difference')\n",
    "    X = train_data[['EloDifference', 'EloSum']]\n",
    "    X = sm.add_constant(X)\n",
    "    y = train_data['pointsDiff']\n",
    "    diff_model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    log('Fitting linear model from Elo difference and sum to points sum')\n",
    "    y = train_data['pointsSum']\n",
    "    sum_model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    log('Generating predictions')\n",
    "#     required_predictions = pd.DataFrame(required_predictions)\n",
    "    tmp = required_predictions[['homeTeam', 'awayTeam']].copy()\n",
    "    tmp['homeElo'] = [elo_dict[team] for team in tmp['homeTeam']]\n",
    "    tmp['awayElo'] = [elo_dict[team] for team in tmp['awayTeam']]\n",
    "    tmp['EloDifference'] = tmp.eval('homeElo - awayElo')\n",
    "    tmp['EloSum'] = tmp.eval('homeElo + awayElo')\n",
    "    X = tmp[['EloDifference', 'EloSum']]\n",
    "    X = sm.add_constant(X)\n",
    "    tmp['predictedDiff'] = diff_model.predict(X)\n",
    "    tmp['predictedSum'] = sum_model.predict(X)\n",
    "    \n",
    "    required_predictions['predictedDiff'] = tmp['predictedDiff']\n",
    "    required_predictions['predictedSum'] = tmp['predictedSum']\n",
    "    \n",
    "    log('Finished')\n",
    "    \n",
    "#     return required_predictions.to_dict('records')\n",
    "    return required_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "required_predictions = test_data[:300][['homeTeam', 'awayTeam', 'dateTime', 'gameId']]\n",
    "required_predictions = predict(required_predictions, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_game_error(predictedDiff, predictedSum, actualDiff, actualSum):\n",
    "    return abs(predictedDiff - actualDiff) + abs(predictedSum - actualSum)\n",
    "\n",
    "def score_predictions(predictions):\n",
    "    x1 = predictions['predictedDiff']\n",
    "    x2 = predictions['predictedSum']\n",
    "    y1 = predictions['pointsDiff']\n",
    "    y2 = predictions['pointsSum']\n",
    "    \n",
    "    ## baseline model \n",
    "    x1_baseline = 0  ## no information about who will win\n",
    "    x2_baseline = 200  ## avergae points total between 2009 and 2016 seasons\n",
    "    \n",
    "    predictions['error'] = single_game_error(x1, x2, y1, y2)\n",
    "    predictions['baseline_error'] = single_game_error(x1_baseline, x2_baseline, y1, y2)\n",
    "    \n",
    "    predictions['score'] = predictions.eval('baseline_error - error')\n",
    "    \n",
    "    return predictions.score.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.merge(required_predictions[['gameId', 'predictedDiff', 'predictedSum']], test_data[['gameId', 'pointsDiff', 'pointsSum']], on='gameId', how='left')\n",
    "score_predictions(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(tmp['score']))\n",
    "plt.xlabel('game')\n",
    "plt.title('cumulative score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simulation\n",
    "settings.predict = predict\n",
    "libsimulation.runSimulation(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
